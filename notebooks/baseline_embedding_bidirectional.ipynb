{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Text to Text Translation : English to French\n",
    "\n",
    "This notebook trains a sequence to sequence (seq2seq) model for English to French translation. This model will be our **baseline** model, which we will then improve upon by adding attention and other features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We will start by importing the libraries we need for this project. You can install any missing libraries using the requirements.txt file provided or by running ``make install`` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport utils.text_processing\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from utils.text_processing import TextProcessor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed, BatchNormalization\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU\n",
    "The following test applies only if you expect to be using a GPU, e.g., while running in a cloud environment with GPU support. Run the next cell, and verify that the device_type is \"GPU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"cuda available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a in depth analysis of the data in the ``exploratory_analysis.ipynb`` notebook. We will not be doing any exploratory analysis in this notebook. Instead, we will focus on building our baseline model. So, let's start by importing the dataset we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Nicolas-BZRD/Parallel_Global_Voices_English_French\", split='train').to_pandas()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual data contains over 350,000 sentence-pairs. However, to speed up training for this notebook, we will only use a small portion of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Use the whole dataset (but it's too big for my computer)\n",
    "dataset = dataset.sample(n=50000, random_state=42)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing\n",
    "\n",
    "The text pre-processing steps will be implemented in a class called ``TextPreprocessor``. This class will be used to clean and tokenize the text data. The class will also be used to convert the text to sequences and pad the sequences to a maximum length. This way we will be able to improve our model's without having to copy and paste the same code over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the english and french sentences\n",
    "dataset['en'] = TextProcessor(dataset, 'en').transform()\n",
    "dataset['fr'] = TextProcessor(dataset, 'fr').transform()\n",
    "\n",
    "# keep only sentences with less than max_sequence_length words\n",
    "dataset = dataset[dataset['en'].str.split().str.len() <= max_sequence_length]\n",
    "dataset = dataset[dataset['fr'].str.split().str.len() <= max_sequence_length]\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Sequence Conversion\n",
    "\n",
    "To feed our data to a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Check the exploratory data analysis notebook to see the distribution of the lengths of the sentences in the dataset. Based on that, we decided to fix the maximum length of each sentence to 20 since the average length of the sentences in the dataset is around 20.\n",
    "\n",
    "We will use the ``Tokenizer`` class from the ``tensorflow.keras.preprocessing.text`` module to tokenize the text data. The ``Tokenizer`` class will also be used to convert the text to sequences. We will use the ``pad_sequences`` function from the same module to pad the sequences to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lines, max_vocab_size=5000):\n",
    "    tokenizer = Tokenizer(filters=' ', num_words=max_vocab_size)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post', truncating='post')\n",
    "    return seq\n",
    "\n",
    "def decode_sequences(tokenizer, sequence):\n",
    "    text = tokenizer.sequences_to_texts([sequence])[0]\n",
    "    text = text.replace('<start>', '').replace('<end>', '').strip()\n",
    "    return text\n",
    "\n",
    "def get_most_common_words(tokenizer, n=10):\n",
    "    word_counts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    return word_counts[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the English sentences\n",
    "eng_tokenizer = tokenization(dataset[\"en\"])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize the French sentences\n",
    "fr_tokenizer = tokenization(dataset[\"fr\"])\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('French Vocabulary Size: %d' % fr_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "We will now split the data into train and test set for model training and evaluation, respectively. We will use the ``train_test_split`` function from the ``sklearn.model_selection`` module to split the data. We will use 10% of the data for testing and the rest for training. We will also set the ``random_state`` parameter to 42 to ensure reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to encode the sentences. We will encode French sentences as the input sequences and English sentences as the target sequences. It will be done for both tra and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fr_tokenizer, max_sequence_length, train_data[\"fr\"])\n",
    "trainY = encode_sequences(eng_tokenizer, max_sequence_length, train_data[\"en\"])\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(fr_tokenizer, max_sequence_length, test_data[\"fr\"])\n",
    "testY = encode_sequences(eng_tokenizer, max_sequence_length, test_data[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode sample sequences from the training set\n",
    "for i in range(1500):\n",
    "    english = decode_sequences(eng_tokenizer, trainY[i, : ])\n",
    "    print('English: ', english, len(english.split()))\n",
    "    french = decode_sequences(fr_tokenizer, trainX[i, :])\n",
    "    print('French: ', french , len(french.split()))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the fun part, building the model. We will build a simple Seq2Seq model for text-to-text translation. \n",
    "The model follows a simple architecture:\n",
    "\n",
    "- Input sequence is embedded using an Embedding layer.\n",
    "- The embedded sequence is processed by an LSTM layer to capture context.\n",
    "- Output sequence is generated by repeating and processing with another LSTM layer.\n",
    "- The Dense layer produces a probability distribution over the output vocabulary for each timestep, enabling text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    embedding_size = 128\n",
    "    \n",
    "    french_input = Input(shape=input_shape[1:], name=\"input_layer\")  # Embedding input (batch, seq_length)\n",
    "    \n",
    "    embeddings = Embedding(input_dim=english_vocab_size, output_dim=embedding_size, \n",
    "                           input_length=output_sequence_length, name=\"Embedding_layer\")(french_input)\n",
    "    \n",
    "    # input shape to LSTM (batchsize, seq_length, embedding_dim) output shape: (batchsize, seq_length, units=64x2)\n",
    "    x = Bidirectional(LSTM(126, return_sequences=True, activation=\"tanh\"), name=\"Bidir_LSTM_layer\")(embeddings)\n",
    "    \n",
    "    preds = TimeDistributed(Dense(french_vocab_size, activation=\"softmax\"), name=\"Dense_layer\")(x)\n",
    "    model = Model(inputs=french_input, outputs=preds, name='Embedding_Bidir_LSTM')\n",
    "       \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/bidirectional.png\"\n",
    "    alt=\"rnn\"\n",
    "    style=\"text-align: center;\" />\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reshape the ``trainX`` and ``trainY`` to be 3-dimensional tensors to be used in the model. The first dimension represents the number of samples (or sentences), the second represents the length of each sequence, and the third represents the number of features in each sequence. We will use the ``trainX`` and ``trainY`` to train the model. We will use the ``testX`` and ``testY`` to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape((-1, max_sequence_length))\n",
    "testX = testX.reshape((-1, max_sequence_length))\n",
    "\n",
    "trainY = trainY.reshape((trainY.shape[0], trainY.shape[1], 1))\n",
    "testY = testY.reshape((testY.shape[0], testY.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using RMSprop optimizer in this model as it is usually a good choice for recurrent neural networks. We will experiment with other optimizers in the next notebook.\n",
    "\n",
    "We will use the ``sparse_categorical_crossentropy`` loss since we have used integers to encode the target sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(trainX.shape, max_sequence_length, 5000, 5000)\n",
    "\n",
    "rms = optimizers.RMSprop(learning_rate=0.0001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have used **sparse_categorical_crossentropy** as the loss function because it allows us to use the target sequence as it is instead of one hot encoded format. One hot encoding the target sequences with such a huge vocabulary might consume our system's entire memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are all set to start training our model. We will train it for **30 epochs** and with a **batch size of 512**. We will also experiment with the hyperparameters in the next notebook.\n",
    "We will also use **ModelCheckpoint()** to save the best model with lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = '../models/embedding_bidirectional.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(trainX, trainY, \n",
    "          epochs=20, batch_size=64,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../models/embedding_bidirectional.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Model\n",
    "\n",
    "Let's compare the training loss and the validation loss. If the validation loss is much higher than the training loss, then the model might be overfitting. We will also evaluate the model on the test set to see how well it performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot evaluation results\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Evaluation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(testX, testY)\n",
    "\n",
    "print(\"Test Loss:\", evaluation[0])\n",
    "print(\"Test Accuracy:\", evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    sentence = encode_sequences(fr_tokenizer, max_sequence_length, [sentence])\n",
    "    prediction = model.predict(sentence.reshape((sentence.shape[0], sentence.shape[1])))\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "    text = decode_sequences(eng_tokenizer, prediction[0])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(reference, candidate):\n",
    "    reference = [reference.split()]\n",
    "    candidate = candidate.split()\n",
    "    return sentence_bleu(reference, candidate)\n",
    "\n",
    "def evaluate_model_bleu_score(test_data):\n",
    "    references = []\n",
    "    candidates = []\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        reference = row['en']\n",
    "        candidate = translate(row['fr'])\n",
    "        \n",
    "        references.append(reference)\n",
    "        candidates.append(candidate)\n",
    "    \n",
    "    return corpus_bleu(references, candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate BLEU score for a single sentence\n",
    "# reference_sentence = \"Hello, how are you?\"\n",
    "# candidate_sentence = \"Bonjour, comment Ã§a va?\"\n",
    "# bleu_score = calculate_bleu_score(reference_sentence, candidate_sentence)\n",
    "# print(\"BLEU score:\", bleu_score)\n",
    "\n",
    "# # Evaluate model BLEU score on test data\n",
    "# test_bleu_score = evaluate_model_bleu_score(test_data)\n",
    "# print(\"Model BLEU score on test data:\", test_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that we have our model, let's make some predictions. We will create a function called ``translate`` which will take a sentence in English as input and return the translated sentence in French. We will use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before let's test on the predictions classes to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_to_predict = 20\n",
    "\n",
    "# Make predictions on the subset\n",
    "subset_to_predict = testX[:size_to_predict]\n",
    "predictions = model.predict_on_batch(subset_to_predict)\n",
    "predictions_classes = np.argmax(predictions, axis=-1)\n",
    "\n",
    "# reshape the subset to predict and the testY to be able to decode them\n",
    "reshapedX_subset = subset_to_predict.reshape((subset_to_predict.shape[0], subset_to_predict.shape[1]))\n",
    "reshapedY_subset = testY[:size_to_predict].reshape((testY[:size_to_predict].shape[0], testY[:size_to_predict].shape[1]))\n",
    "\n",
    "predicted_df = pd.DataFrame(columns=['french_sentence', 'actual_english_sentence', 'predicted_english_sentence'])\n",
    "\n",
    "i = 0\n",
    "for seq in predictions_classes:\n",
    "    predicted_text = decode_sequences(eng_tokenizer, seq)\n",
    "    original_french_sentence = decode_sequences(fr_tokenizer, reshapedX_subset[i])\n",
    "    original_english_sentence = decode_sequences(eng_tokenizer, reshapedY_subset[i])\n",
    "    \n",
    "    predicted_df.loc[i] = [original_french_sentence, original_english_sentence, predicted_text]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some predictions, with the ``translate`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "data = []\n",
    "\n",
    "references = []\n",
    "candidates = []\n",
    "\n",
    "for i in tqdm(range(3000)):\n",
    "    textX_decoded = decode_sequences(fr_tokenizer, testX[i,])\n",
    "    testY_decoded = decode_sequences(eng_tokenizer, testY[i, : ,0])\n",
    "    candidate = translate(textX_decoded).replace('<end>', '').replace('<start>', '').strip()\n",
    "    \n",
    "    data.append({\n",
    "        'Context': textX_decoded,\n",
    "        'Reference': testY_decoded,\n",
    "        'Candidate': candidate,\n",
    "        'length': len(textX_decoded.split())\n",
    "    })\n",
    "    \n",
    "    references.append([testY_decoded])\n",
    "    candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into small dataset based on the sentences length\n",
    "length_ranges = [(1, 5), (6, 10), (11, 15), (16, 20), (21, 30), (31, 40), (41, 60), (61, float('inf'))]\n",
    "\n",
    "small_datasets = {}\n",
    "for min_len, max_len in length_ranges:\n",
    "    filtered_examples = [example for example in data if example['length'] >= min_len and example['length'] <= max_len]\n",
    "    small_datasets[f'dataset_{min_len}_{max_len}'] = filtered_examples\n",
    "\n",
    "samples_per_range = []\n",
    "for key, dataset in small_datasets.items():\n",
    "    samples_per_range.append(len(dataset))\n",
    "    print(f\"{key}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corpus_bleu(references, candidates):\n",
    "    if len(references) != len(candidates):\n",
    "        raise ValueError('The number of references and candidates must be the same :', len(references), len(candidates))\n",
    "    \n",
    "    if len(references) == 0: return 0.0\n",
    "    \n",
    "    reference_tokens = [[ref] for ref in references]\n",
    "    return corpus_bleu(reference_tokens, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = []\n",
    "for key, dataset in small_datasets.items():\n",
    "    refs = [example['Reference'] for example in dataset]\n",
    "    cands = [example['Candidate'] for example in dataset]\n",
    "    \n",
    "    corpus_bleu_score = compute_corpus_bleu(refs, cands)\n",
    "    bleu_scores.append(corpus_bleu_score)\n",
    "    \n",
    "    print(f\"{key}: {corpus_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_bleu_score = corpus_bleu(references, candidates)\n",
    "overall_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  # List of colors for each bar\n",
    "bar_plot = plt.bar([f'{start}-{end}' for start, end in length_ranges], bleu_scores, color=colors, alpha=0.7, label='BLEU Score')\n",
    "\n",
    "# Add \"All\" bar with legend\n",
    "all_bar = plt.bar(\"All\", overall_bleu_score * 2, color='k', alpha=0.7)\n",
    "\n",
    "# Create a dummy handle for the \"All\" bar\n",
    "all_patch = mpatches.Patch(color='k', label=f'Sample = {len(candidates)}')\n",
    "legend_labels = [f'Sample = {value}' for value in samples_per_range]\n",
    "\n",
    "# Include the dummy handle in the legend\n",
    "plt.legend(handles=[*bar_plot, all_patch], labels=legend_labels + [f'Sample = {len(candidates)}'], loc='upper right', title='Samples per range')\n",
    "\n",
    "plt.xlabel('Word Count Range')\n",
    "plt.ylabel('BLEU Score')\n",
    "\n",
    "plt.title('BLEU Score and Number of Samples Based on Word Count Range')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
