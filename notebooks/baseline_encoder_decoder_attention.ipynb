{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22731e38",
   "metadata": {},
   "source": [
    "# Attention Mechanisme Text to Text Translation : English to French\n",
    "\n",
    "This notebook trains a model for French to English translation. Using the attention mechanism, the model learns to align and translate French text to English text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47fcecf",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We will start by importing the libraries we need for this project. You can install any missing libraries using the requirements.txt file provided or by running ``make install`` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb64c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport utils.text_processing\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-gilbert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import digits\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from utils.text_processing import TextProcessor\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f9519",
   "metadata": {},
   "source": [
    "### Verify access to the GPU\n",
    "The following test applies only if you expect to be using a GPU, e.g., while running in a cloud environment with GPU support. Run the next cell, and verify that the device_type is \"GPU\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d302be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"cuda available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9863892",
   "metadata": {},
   "source": [
    "We provide a in depth analysis of the data in the ``exploratory_analysis.ipynb`` notebook. We will not be doing any exploratory analysis in this notebook. Instead, we will focus on building our baseline model. So, let's start by importing the dataset we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326899b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Nicolas-BZRD/Parallel_Global_Voices_English_French\", split='train').to_pandas()\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd72c8",
   "metadata": {},
   "source": [
    "The actual data contains over 350,000 sentence-pairs. However, to speed up training for this notebook, we will only use a small portion of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Use the whole dataset (but it's too big for my computer)\n",
    "dataset = dataset.sample(n=50000, random_state=42)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-crash",
   "metadata": {},
   "source": [
    "## Text Pre-Processing\n",
    "\n",
    "The text pre-processing steps will be implemented in a class called ``TextPreprocessor``. This class will be used to clean and tokenize the text data. The class will also be used to convert the text to sequences and pad the sequences to a maximum length. This way we will be able to improve our model's without having to copy and paste the same code over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7168245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['en'] = TextProcessor(dataset, 'en').transform()\n",
    "dataset['fr'] = TextProcessor(dataset, 'fr').transform()\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c032ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only sentences with less than max_sequence_length words\n",
    "dataset = dataset[dataset['en'].str.split().str.len() <= max_sequence_length]\n",
    "dataset = dataset[dataset['fr'].str.split().str.len() <= max_sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e7bc9",
   "metadata": {},
   "source": [
    "### Text to Sequence Conversion\n",
    "\n",
    "To feed our data to a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Check the exploratory data analysis notebook to see the distribution of the lengths of the sentences in the dataset. Based on that, we decided to fix the maximum length of each sentence to 20 since the average length of the sentences in the dataset is around 20.\n",
    "\n",
    "We will use the ``Tokenizer`` class from the ``tensorflow.keras.preprocessing.text`` module to tokenize the text data. The ``Tokenizer`` class will also be used to convert the text to sequences. We will use the ``pad_sequences`` function from the same module to pad the sequences to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082de051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lines, max_vocab_size=100000):\n",
    "    tokenizer = Tokenizer(filters=' ', num_words=max_vocab_size)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post', truncating='post')\n",
    "    return seq\n",
    "\n",
    "def decode_sequences(tokenizer, sequence):\n",
    "    text = tokenizer.sequences_to_texts([sequence])[0]\n",
    "    text = text.replace('<start>', '').replace('<end>', '').strip()\n",
    "    return text\n",
    "\n",
    "def get_most_common_words(tokenizer, n=10):\n",
    "    word_counts = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    return word_counts[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9445480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the English sentences\n",
    "eng_tokenizer = tokenization(dataset[\"en\"], max_vocab_size=max_vocab_size)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize the French sentences\n",
    "fr_tokenizer = tokenization(dataset[\"fr\"], max_vocab_size=max_vocab_size)\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('French Vocabulary Size: %d' % fr_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common words in English: \", get_most_common_words(eng_tokenizer))\n",
    "print(\"Most common words in French: \", get_most_common_words(fr_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ea319c",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "We will now split the data into train and test set for model training and evaluation, respectively. We will use the ``train_test_split`` function from the ``sklearn.model_selection`` module to split the data. We will use 10% of the data for testing and the rest for training. We will also set the ``random_state`` parameter to 42 to ensure reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae478a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8cc1a",
   "metadata": {},
   "source": [
    "It's time to encode the sentences. We will encode French sentences as the input sequences and English sentences as the target sequences. It will be done for both tra and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fr_tokenizer, max_sequence_length, dataset[\"fr\"])\n",
    "trainY = encode_sequences(eng_tokenizer, max_sequence_length, dataset[\"en\"])\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(fr_tokenizer, max_sequence_length, test_data[\"fr\"])\n",
    "testY = encode_sequences(eng_tokenizer, max_sequence_length, test_data[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape((-1, trainX.shape[1], 1))\n",
    "testX = testX.reshape((-1, testX.shape[1], 1))\n",
    "\n",
    "trainY = trainY.reshape((-1, trainY.shape[1], 1))\n",
    "testY = testY.reshape((-1, testY.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a3f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7564baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode sample sequences from the training set\n",
    "for i in range(1500):\n",
    "    english = decode_sequences(eng_tokenizer, trainY[i, : ,0])\n",
    "    french = decode_sequences(fr_tokenizer, trainX[i, : ,0])\n",
    "    print('English: ', english, len(english.split()))\n",
    "    print('French: ', french , len(french.split()))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-weather",
   "metadata": {},
   "source": [
    "## Encoder Decoder with Attention mechanism \n",
    "\n",
    "![Attention](../images/attention.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(encoder_units, \n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,                                      \n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # pass the input x to the embedding layer\n",
    "        x = self.embedding(x)\n",
    "        # pass the embedding and the hidden state to GRU\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoder_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        \n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        # hidden state shape == (batch_size, hidden size)\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # enc_output (batch_size, max?_lenght, encoder_units) ,enc_hidden (batch_size, encoder_units)\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden) \n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # dec_input(batch_size, 1)\n",
    "        dec_input = tf.expand_dims([fr_tokenizer.word_index['<start>']] * BATCH_SIZE, 1) \n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1) # using teacher forcing\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "# Create data in memeory and shuffles the data in the batches\n",
    "dataset=tf.data.Dataset.from_tensor_slices((trainX.reshape((-1, trainX.shape[-2])), trainY.reshape((-1, trainY.shape[1])))).shuffle(BATCH_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(fr_vocab_size, embedding_dim=embedding_dim, encoder_units=units, batch_size=BATCH_SIZE)\n",
    "decoder = Decoder(vocab_size=eng_vocab_size, embedding_dim=embedding_dim, dec_units=units, batch_sz=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '../models/encode_decoder_attention_training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "steps_per_epoch = len(trainX) // BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in tqdm(enumerate(dataset.take(steps_per_epoch)), total=steps_per_epoch):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-catalyst",
   "metadata": {},
   "source": [
    "### Make Prediction using the trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentences, source_sentence_tokenizer=fr_tokenizer, target_sentence_tokenizer=eng_tokenizer): \n",
    "    max_length = trainX.shape[1]\n",
    "    attention_plot = np.zeros((max_length, max_length))\n",
    "\n",
    "    sentence = TextProcessor(sentences, 'fr').process(sentences)\n",
    "\n",
    "    inputs = [source_sentence_tokenizer.word_index[w] for w in sentences.split()]\n",
    "    inputs = pad_sequences([inputs], maxlen=max_length, padding=\"post\")\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = \"\"\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_sentence_tokenizer.word_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights =  tf.reshape(attention_weights,(-1, ))\n",
    "        attention_plot [t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += target_sentence_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if target_sentence_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap=\"viridis\")\n",
    "    \n",
    "    sentence = sentence.split()\n",
    "    predicted_sentence = predicted_sentence.split()\n",
    "    \n",
    "    # Add <PAD> token if sentence or predicted_sentence is shorter than attention matrix\n",
    "    if len(sentence) < attention.shape[1]:\n",
    "        sentence += [\"<PAD>\"] * (attention.shape[1] - len(sentence))\n",
    "        \n",
    "    if len(predicted_sentence) < attention.shape[0]:\n",
    "        predicted_sentence += [\"<PAD>\"] * (attention.shape[0] - len(predicted_sentence))\n",
    "    \n",
    "    fontdict = {\"fontsize\": 14}\n",
    "    ax.set_xticklabels([' '] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([' '] + predicted_sentence, fontdict=fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    predicted_text, sentence, attention_plot = predict(sentence)\n",
    "        \n",
    "    attention_plot = attention_plot[:len(predicted_text.split(' ')), :len(sentence.split(' '))]\n",
    "    return predicted_text, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = np.random.randint(0, testX.shape[0])\n",
    "sentence = ' '.join([fr_tokenizer.index_word[i[0]] for i in testX[random_id] if i[0] != 0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_text, attention_plot = translate(sentence)\n",
    "\n",
    "print('Input:        ', sentence)\n",
    "print('Predicted:   ', predicted_text)\n",
    "print('Ground Truth: ', decode_sequences(eng_tokenizer, testY[random_id, : ,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86687643",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(attention_plot, sentence, predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c7816",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "references = []\n",
    "candidates = []\n",
    "\n",
    "for i in tqdm(range(testX.shape[0]//2)):\n",
    "    textX_decoded = decode_sequences(fr_tokenizer, testX[i, : ,0])\n",
    "    testY_decoded = decode_sequences(eng_tokenizer, testY[i, : ,0])\n",
    "    candidate = translate(textX_decoded)[0].replace('<end>', '').replace('<start>', '').strip()\n",
    "    \n",
    "    data.append({\n",
    "        'Context': textX_decoded,\n",
    "        'Reference': testY_decoded,\n",
    "        'Candidate': candidate,\n",
    "        'length': len(textX_decoded.split())\n",
    "    })\n",
    "    \n",
    "    references.append([testY_decoded])\n",
    "    candidates.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into small dataset based on the sentences length\n",
    "length_ranges = [(1, 5), (6, 10), (11, 15), (16, 20), (21, 30), (31, 40), (41, 60), (61, float('inf'))]\n",
    "\n",
    "small_datasets = {}\n",
    "for min_len, max_len in length_ranges:\n",
    "    filtered_examples = [example for example in data if example['length'] >= min_len and example['length'] <= max_len]\n",
    "    small_datasets[f'dataset_{min_len}_{max_len}'] = filtered_examples\n",
    "\n",
    "samples_per_range = []\n",
    "for key, dataset in small_datasets.items():\n",
    "    samples_per_range.append(len(dataset))\n",
    "    print(f\"{key}: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5089eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corpus_bleu(references, candidates):\n",
    "    if len(references) != len(candidates):\n",
    "        raise ValueError('The number of references and candidates must be the same :', len(references), len(candidates))\n",
    "    \n",
    "    if len(references) == 0: return 0.0\n",
    "    \n",
    "    reference_tokens = [[ref] for ref in references]\n",
    "    return corpus_bleu(reference_tokens, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7194b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_scores = []\n",
    "for key, dataset in small_datasets.items():\n",
    "    refs = [example['Reference'] for example in dataset]\n",
    "    cands = [example['Candidate'] for example in dataset]\n",
    "    \n",
    "    corpus_bleu_score = compute_corpus_bleu(refs, cands)\n",
    "    bleu_scores.append(corpus_bleu_score)\n",
    "    \n",
    "    print(f\"{key}: {corpus_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ca8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_bleu_score = corpus_bleu(references, candidates)\n",
    "overall_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']  # List of colors for each bar\n",
    "bar_plot = plt.bar([f'{start}-{end}' for start, end in length_ranges], bleu_scores, color=colors, alpha=0.7, label='BLEU Score')\n",
    "\n",
    "# Add \"All\" bar with legend\n",
    "all_bar = plt.bar(\"All\", overall_bleu_score, color='k', alpha=0.7)\n",
    "\n",
    "# Create a dummy handle for the \"All\" bar\n",
    "all_patch = mpatches.Patch(color='k', label=f'Sample = {len(candidates)}')\n",
    "legend_labels = [f'Sample = {value}' for value in samples_per_range]\n",
    "\n",
    "# Include the dummy handle in the legend\n",
    "plt.legend(handles=[*bar_plot, all_patch], labels=legend_labels + [f'Sample = {len(candidates)}'], loc='upper right', title='Samples per range')\n",
    "\n",
    "plt.xlabel('Word Count Range')\n",
    "plt.ylabel('BLEU Score')\n",
    "\n",
    "plt.title('BLEU Score and Number of Samples Based on Word Count Range')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
