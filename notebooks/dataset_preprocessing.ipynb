{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Text Translation Preprocessing\n",
    "\n",
    "This notebook aims to preprocess the Parallel Global Voices English-French dataset for text-to-text translation tasks.\n",
    "\n",
    "## Import Libraries\n",
    "\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from transformers import MarianTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the Parallel Global Voices English-French dataset using the `load_dataset` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Nicolas-BZRD/Parallel_Global_Voices_English_French\", split='train').to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Steps\n",
    "\n",
    "### Text Cleaning\n",
    "\n",
    "The text cleaning steps are as follows:\n",
    "- Lowercase the text\n",
    "- Remove special characters (we will keep the punctuation marks as they are important for translation tasks)\n",
    "- handle URLs/HTML tags, etc.\n",
    "- Remove extra whitespaces\n",
    "- Expand contractions (e.g. don't -> do not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = text.lower()                 # lowercase\n",
    "    text = re.sub('<[^>]+>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
    "    text = re.sub(' +', ' ', text)      # remove extra spaces\n",
    "    \n",
    "    # TODO : remove special characters (check the exploratory notebook)\n",
    "    # TODO : expand contractions (here or inside a data augmentation function \"import contractions\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['en_cleaned'] = dataset['en'].apply(lambda x: text_cleaning(x))\n",
    "dataset['fr_cleaned'] = dataset['fr'].apply(lambda x: text_cleaning(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Numerical Encoding\n",
    "In this step, we perform tokenization and numerical encoding using an appropriate tokenizer. We use the MarianTokenizer from the transformers library, specifically the 'Helsinki-NLP/opus-mt-en-fr' model, which is trained for English to French translation.\n",
    "\n",
    "The ``dataset['en_cleaned']`` column contains the preprocessed English text, and the ``dataset['fr_cleaned']`` column contains the preprocessed French text. We apply the tokenizer.encode() method to each text, which tokenizes the text and returns the encoded representation as tensors.\n",
    "\n",
    "The encoded representations are stored in the ``dataset['en_encoded']`` and ``dataset['fr_encoded']`` columns, respectively. These encoded representations can be used as input to train translation models or perform other text-to-text translation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and numerical encoding using appropriate tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
    "dataset['en_encoded'] = dataset['en_cleaned'].apply(lambda x: tokenizer.encode(x, return_tensors='pt'))\n",
    "dataset['fr_encoded'] = dataset['fr_cleaned'].apply(lambda x: tokenizer.encode(x, return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and Truncation\n",
    "\n",
    "This section performs padding and truncation on the encoded sequences to ensure they have a consistent length.\n",
    "The maximum sequence length is set to 512.\n",
    "Sequences shorter than the maximum length are padded with zeros, while longer sequences are truncated.\n",
    "The padded and truncated sequences are stored in the 'en_padded' and 'fr_padded' columns of the dataset, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512  # Maximum sequence length supported by the model\n",
    "\n",
    "# Tokenization and numerical encoding with truncation and padding\n",
    "dataset['en_encoded'] = dataset['en_cleaned'].apply(lambda x: tokenizer.encode(x, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='pt'))\n",
    "dataset['fr_encoded'] = dataset['fr_cleaned'].apply(lambda x: tokenizer.encode(x, max_length=max_seq_length, truncation=True, padding='max_length', return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.to_csv('../data/preprocessed/preprocessed_train.csv', index=False)\n",
    "# test_data.to_csv('../data/preprocessed/preprocessed_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
