{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Text to Text Translation : English to French\n",
    "\n",
    "This notebook trains a sequence to sequence (seq2seq) model for English to French translation. This model will be our **baseline** model, which we will then improve upon by adding attention and other features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We will start by importing the libraries we need for this project. You can install any missing libraries using the requirements.txt file provided or by running ``make install`` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrateur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrateur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a in depth analysis of the data in the ``exploratory_analysis.ipynb`` notebook. We will not be doing any exploratory analysis in this notebook. Instead, we will focus on building our baseline model. So, let's start by importing the dataset we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jamaica: “I am HIV”</td>\n",
       "      <td>Jamaïque : J’ai le VIH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's widely acknowledged, in the Caribbean and...</td>\n",
       "      <td>Il est largement reconnu, dans les Caraïbes et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For this woman, however, photographed in the s...</td>\n",
       "      <td>Pour cette femme, cependant, photographiée dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As Bacon writes on her blog:</td>\n",
       "      <td>Comme Bacon écrit sur son blog:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“When I asked to take her picture, I suggested...</td>\n",
       "      <td>“Quand je lui ai demandé de la prendre en phot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                                Jamaica: “I am HIV”   \n",
       "1  It's widely acknowledged, in the Caribbean and...   \n",
       "2  For this woman, however, photographed in the s...   \n",
       "3                       As Bacon writes on her blog:   \n",
       "4  “When I asked to take her picture, I suggested...   \n",
       "\n",
       "                                                  fr  \n",
       "0                             Jamaïque : J’ai le VIH  \n",
       "1  Il est largement reconnu, dans les Caraïbes et...  \n",
       "2  Pour cette femme, cependant, photographiée dan...  \n",
       "3                    Comme Bacon écrit sur son blog:  \n",
       "4  “Quand je lui ai demandé de la prendre en phot...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"Nicolas-BZRD/Parallel_Global_Voices_English_French\", split='train').to_pandas()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual data contains over 350,000 sentence-pairs. However, to speed up training for this notebook, we will only use a small portion of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(563, 2)\n"
     ]
    }
   ],
   "source": [
    "# TODO : Use the whole dataset (but it's too big for my computer)\n",
    "dataset = dataset.sample(n=1000, random_state=42)\n",
    "print(dataset.shape)\n",
    "\n",
    "# TODO : move inside the preprocessing step\n",
    "en_len = dataset['en'].str.count(' ')\n",
    "fr_len = dataset['fr'].str.count(' ')\n",
    "\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "dataset = dataset[(en_len <= MAX_LENGTH) & (fr_len <= MAX_LENGTH)]\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-Processing\n",
    "\n",
    "The text pre-processing steps will be implemented in a class called ``TextPreprocessor``. This class will be used to clean and tokenize the text data. The class will also be used to convert the text to sequences and pad the sequences to a maximum length. This way we will be able to improve our model's without having to copy and paste the same code over and over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Sequence Conversion\n",
    "\n",
    "To feed our data to a Seq2Seq model, we will have to convert both the input and the output sentences into integer sequences of fixed length. Check the exploratory data analysis notebook to see the distribution of the lengths of the sentences in the dataset. Based on that, we decided to fix the maximum length of each sentence to 20 since the average length of the sentences in the dataset is around 20.\n",
    "\n",
    "We will use the ``Tokenizer`` class from the ``tensorflow.keras.preprocessing.text`` module to tokenize the text data. The ``Tokenizer`` class will also be used to convert the text to sequences. We will use the ``pad_sequences`` function from the same module to pad the sequences to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq\n",
    "\n",
    "def decode_sequences(tokenizer, sequence):\n",
    "    text = tokenizer.sequences_to_texts([sequence])[0].replace('PAD', '').strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the English sentences\n",
    "eng_tokenizer = tokenization(dataset[\"en\"])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "# Tokenize the French sentences\n",
    "fr_tokenizer = tokenization(dataset[\"fr\"])\n",
    "fr_vocab_size = len(fr_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2562\n",
      "French Vocabulary Size: 2814\n"
     ]
    }
   ],
   "source": [
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('French Vocabulary Size: %d' % fr_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "We will now split the data into train and test set for model training and evaluation, respectively. We will use the ``train_test_split`` function from the ``sklearn.model_selection`` module to split the data. We will use 10% of the data for testing and the rest for training. We will also set the ``random_state`` parameter to 42 to ensure reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to encode the sentences. We will encode French sentences as the input sequences and English sentences as the target sequences. It will be done for both tra and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fr_tokenizer, max_sequence_length, train_data[\"fr\"])\n",
    "trainY = encode_sequences(eng_tokenizer, max_sequence_length, train_data[\"en\"])\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(fr_tokenizer, max_sequence_length, test_data[\"fr\"])\n",
    "testY = encode_sequences(eng_tokenizer, max_sequence_length, test_data[\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the fun part, building the model. We will build a simple Seq2Seq model for text-to-text translation. \n",
    "The model follows a simple architecture:\n",
    "\n",
    "- Input sequence is embedded using an Embedding layer.\n",
    "- The embedded sequence is processed by an LSTM layer to capture context.\n",
    "- Output sequence is generated by repeating and processing with another LSTM layer.\n",
    "- The Dense layer produces a probability distribution over the output vocabulary for each timestep, enabling text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    \n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using RMSprop optimizer in this model as it is usually a good choice for recurrent neural networks. We will experiment with other optimizers in the next notebook.\n",
    "\n",
    "We will use the ``sparse_categorical_crossentropy`` loss since we have used integers to encode the target sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(fr_vocab_size, eng_vocab_size, max_sequence_length, max_sequence_length, 64)\n",
    "rms = optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have used **sparse_categorical_crossentropy** as the loss function because it allows us to use the target sequence as it is instead of one hot encoded format. One hot encoding the target sequences with such a huge vocabulary might consume our system's entire memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we are all set to start training our model. We will train it for **30 epochs** and with a **batch size of 512**. We will also experiment with the hyperparameters in the next notebook.\n",
    "We will also use **ModelCheckpoint()** to save the best model with lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = '../models/test.md'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
    "          epochs=10, batch_size=512, \n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrateur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Administrateur\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:229: The name tf.Dimension is deprecated. Please use tf.compat.v1.Dimension instead.\n",
      "\n",
      "4/4 [==============================] - 1s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../models/test.md')\n",
    "predictions = model.predict(testX.reshape((testX.shape[0], testX.shape[1])))\n",
    "predictions_classes = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Model\n",
    "\n",
    "Let's compare the training loss and the validation loss. If the validation loss is much higher than the training loss, then the model might be overfitting. We will also evaluate the model on the test set to see how well it performs on unseen data.\n",
    "\n",
    "**TODO:** \n",
    "- Add BLEU score\n",
    "- Add attention (if we implemented the attention mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrateur\\Documents\\Github\\text-to-text-translation\\notebooks\\baseline.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/Github/text-to-text-translation/notebooks/baseline.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(model\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mloss\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/Github/text-to-text-translation/notebooks/baseline.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(model\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrateur/Documents/Github/text-to-text-translation/notebooks/baseline.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mlegend([\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that we have our model, let's make some predictions. We will create a function called ``translate`` which will take a sentence in English as input and return the translated sentence in French. We will use the trained model to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    sentence = encode_sequences(fr_tokenizer, max_sequence_length, [sentence])\n",
    "    prediction = model.predict(sentence.reshape((sentence.shape[0], sentence.shape[1])))\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "    text = decode_sequences(eng_tokenizer, prediction[0])\n",
    "    print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before let's test on the predictions classes to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text = []\n",
    "\n",
    "for seq in predictions_classes:\n",
    "    predicted_text = decode_sequences(eng_tokenizer, seq)\n",
    "    preds_text.append(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334656</th>\n",
       "      <td>We are all struggling to live and we are always busy with our lives.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134625</th>\n",
       "      <td>Here's how it went and how the Lebanese on line community reacted to this move.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173850</th>\n",
       "      <td>Those publications are neither “handate messages” nor ” anti-peace”…</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72208</th>\n",
       "      <td>Japan: I want my husband dead</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192076</th>\n",
       "      <td>Where will we stand?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149004</th>\n",
       "      <td>Photo by Julio Albarrán, republished under a CC License. *</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222591</th>\n",
       "      <td>One of the many responses to the story is by Mungoma and reads:</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>But, the car came back and ran over him killing him instantly.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48725</th>\n",
       "      <td>Gameron writes [fa] that homosexuals face problems in Islamic countries where they can be executed.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230044</th>\n",
       "      <td>I talked to Wong Tack during the break.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165275</th>\n",
       "      <td>Like last year, a number of social initiatives have been launched to help out the displaced flood affectees.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71492</th>\n",
       "      <td>Féroce Remanongona, an elected official goes further as to say (fr):</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69623</th>\n",
       "      <td>In their words: Women tell of their children and prison life</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247373</th>\n",
       "      <td>That fire burns us all.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130618</th>\n",
       "      <td>Young Copt wounded in revolution - arm in sling - sings about Jesus Christ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              actual  \\\n",
       "334656                                          We are all struggling to live and we are always busy with our lives.   \n",
       "134625                               Here's how it went and how the Lebanese on line community reacted to this move.   \n",
       "173850                                          Those publications are neither “handate messages” nor ” anti-peace”…   \n",
       "72208                                                                                  Japan: I want my husband dead   \n",
       "192076                                                                                          Where will we stand?   \n",
       "149004                                                    Photo by Julio Albarrán, republished under a CC License. *   \n",
       "222591                                               One of the many responses to the story is by Mungoma and reads:   \n",
       "2727                                                  But, the car came back and ran over him killing him instantly.   \n",
       "48725            Gameron writes [fa] that homosexuals face problems in Islamic countries where they can be executed.   \n",
       "230044                                                                       I talked to Wong Tack during the break.   \n",
       "165275  Like last year, a number of social initiatives have been launched to help out the displaced flood affectees.   \n",
       "71492                                           Féroce Remanongona, an elected official goes further as to say (fr):   \n",
       "69623                                                   In their words: Women tell of their children and prison life   \n",
       "247373                                                                                       That fire burns us all.   \n",
       "130618                                    Young Copt wounded in revolution - arm in sling - sings about Jesus Christ   \n",
       "\n",
       "       predicted  \n",
       "334656            \n",
       "134625            \n",
       "173850            \n",
       "72208             \n",
       "192076            \n",
       "149004            \n",
       "222591            \n",
       "2727              \n",
       "48725             \n",
       "230044            \n",
       "165275            \n",
       "71492             \n",
       "69623             \n",
       "247373            \n",
       "130618            "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test_data[\"en\"], 'predicted' : preds_text})\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pred_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make some predictions, with the ``translate`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(\"Bonjour\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
